{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db52c98b-f413-4ac2-a553-09e9adcb53e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import copy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bad9a709-17ac-4771-a024-93abdb90b330",
   "metadata": {},
   "source": [
    "# Linear Regression with L2 Regularization\n",
    "\n",
    "### Introduction\n",
    "This includes the documentation of the linear regression model with L2 regularization implemented in this notebook.\n",
    "\n",
    "The cost function used for linear regression is\n",
    "$$\n",
    "J(\\vec{w}, b) = \\frac{1}{2m} \\sum_{i=0}^{m-1} \\left( f_{\\vec{w},b}(\\vec{x}^{(i)}) - y^{(i)} \\right)^2 + \\frac{\\lambda}{2m} \\sum_{j=0}^{n-1} w_{j} ^ 2\n",
    "$$\n",
    "where *m* is the number of training examples, *n* is the number of features, and \\\\(\\lambda\\\\) is the regularization parameter.\n",
    "\n",
    "### Gradient Descent Algorithm\n",
    "\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "\\text{Repeat until convergence} \\\\\n",
    "\\left\\{ \\begin{array}{c}\n",
    "    w_j = w_j - \\alpha \\frac{\\partial}{\\partial w_j} J(\\vec{w}, b) \\\\\n",
    "    b = b - \\alpha \\frac{\\partial}{\\partial b} J(\\vec{w}, b)\n",
    "\\end{array} \\right.\n",
    "\\end{array}\n",
    "$$\n",
    "where **w**, b are updated simultaneously.\n",
    "\n",
    "The gradient is defined as:\n",
    "$$\n",
    "\\frac{\\partial J(\\vec{w}, b)}{\\partial w_{j}} = \\frac{1}{m} \\sum_{i=0}^{m-1} \\left( f_{\\vec{w},b}(\\vec{x}^{(i)}) - y^{(i)} \\right) x_{j}^{(i)} + \\frac{\\lambda}{m}  w_{j}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(w, b)}{\\partial b} = \\frac{1}{m} \\sum_{i=0}^{m-1} \\left( f_{\\vec{w},b}(\\vec{x}^{(i)}) - y^{(i)} \\right)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e7758d-e2e7-4721-bcd9-f649ee9300d0",
   "metadata": {},
   "source": [
    "## Test cases\n",
    "1. Empty dataset (m = 0)\n",
    "2. Single training example (m = 1)\n",
    "3. Single feature (n = 1)\n",
    "4. Weights are zero (w = [0, 0, ...])\n",
    "5. High regularization factor (\\\\(\\lambda\\\\))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19280b59-2366-42c1-b73f-a305c91fba72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing compute_cost function\n",
    "\n",
    "# Test 1: Empty Dataset\n",
    "X_test1 = np.array([])\n",
    "y_test1 = np.array([])\n",
    "w_test1 = np.array([])\n",
    "b_test1 = 0\n",
    "lambda_test1 = 1\n",
    "print(compute_cost(X_test1.reshape(0, 0), y_test1, w_test1, b_test1, lambda_test1))  # Should return 0.0\n",
    "\n",
    "# Test 2: Single Training Example\n",
    "X_test2 = np.array([[1]])\n",
    "y_test2 = np.array([1])\n",
    "w_test2 = np.array([1])\n",
    "b_test2 = 1\n",
    "lambda_test2 = 1\n",
    "print(compute_cost(X_test2, y_test2, w_test2, b_test2, lambda_test2))\n",
    "\n",
    "# Test 3: Single Feature\n",
    "X_test3 = np.array([[1], [2], [3]])\n",
    "y_test3 = np.array([1, 2, 3])\n",
    "w_test3 = np.array([1])\n",
    "b_test3 = 1\n",
    "lambda_test3 = 1\n",
    "print(compute_cost(X_test3, y_test3, w_test3, b_test3, lambda_test3))\n",
    "\n",
    "# Test 4: Weights are Zero\n",
    "X_test4 = np.array([[1, 2], [3, 4]])\n",
    "y_test4 = np.array([1, 2])\n",
    "w_test4 = np.array([0, 0])\n",
    "b_test4 = 1\n",
    "lambda_test4 = 1\n",
    "print(compute_cost(X_test4, y_test4, w_test4, b_test4, lambda_test4))\n",
    "\n",
    "# Test 5: High Regularization Factor\n",
    "X_test5 = np.array([[1, 2], [3, 4]])\n",
    "y_test5 = np.array([1, 2])\n",
    "w_test5 = np.array([1, 1])\n",
    "b_test5 = 1\n",
    "lambda_test5 = 100\n",
    "print(compute_cost(X_test5, y_test5, w_test5, b_test5, lambda_test5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5108e754-3a44-4929-b6df-8c65b8a39853",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, w, b, lambda_):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    X : ndarray, shape [m, n] \n",
    "        Training Dataset of m examples and n features.\n",
    "    y: ndarray, shape [m,]    \n",
    "        Target values.\n",
    "    w: ndarray, shape [n,]    \n",
    "        Model Parameters (weights).\n",
    "    b: scalar  \n",
    "        Model Parameter (bias).\n",
    "    lambda_ : scalar\n",
    "        Regularization factor.\n",
    "\n",
    "    Returns:\n",
    "    total_cost: scalar \n",
    "        The total cost including the squared error cost and regularization cost.\n",
    "    \"\"\"\n",
    "    \n",
    "    m, _ = X.shape \n",
    "    if m == 0:\n",
    "        return 0.\n",
    "       \n",
    "    # Cost without regularization\n",
    "    unreg_cost = np.sum(((np.dot(X, w) + b) - y)**2)  / (2*m)\n",
    "    print(f\"Custom - Unregularized Cost: {unreg_cost}\")\n",
    "\n",
    "    # Cost added by regularization (Ridge Regression) \n",
    "    reg_cost = (lambda_ / (2*m)) * np.sum(w**2)    \n",
    "    print(f\"Custom - Regularization Cost: {reg_cost}\")\n",
    "\n",
    "    # Total cost\n",
    "    total_cost = unreg_cost + reg_cost \n",
    "    print(f\"Custom - Total Cost: {total_cost}\")\n",
    "    \n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028e4902-e818-4e95-8a65-a2178d7a9d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "y = np.array([1, 2, 3]) \n",
    "w = np.array([0.5, 0.5]) \n",
    "b = 0.5 \n",
    "lambda_ = 0.1\n",
    "compute_cost(X, y, w, b, lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a9e334-2c78-445e-95e6-974dc93b7781",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(X, y, w, b, lambda_):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    X : ndarray, shape [m, n] \n",
    "        Training Dataset of m examples and n features.\n",
    "    y: ndarray, shape [m,]    \n",
    "        Target values.\n",
    "    w: ndarray, shape [n,]    \n",
    "        Model Parameters (weights).\n",
    "    b: scalar  \n",
    "        Model Parameter (bias).\n",
    "    lambda_ : scalar\n",
    "        Regularization factor.\n",
    "\n",
    "    Returns:\n",
    "    dj_dw: ndarray, shape (n,)\n",
    "        Gradients of cost function w.r.t. weight parameters.\n",
    "    dj_db: scalar\n",
    "        Gradient of cost function w.r.t. bias.\n",
    "    \"\"\"\n",
    "    \n",
    "    m, n = X.shape\n",
    "    if m == 0:\n",
    "        return np.zeros(n), 0.0\n",
    "\n",
    "    # Computes the error matrix of [m,] dimension.\n",
    "    errors = (np.dot(X, w) + b) - y\n",
    "\n",
    "    #Computet gradients\n",
    "    dj_dw = np.sum(errors.reshape(-1, 1) * X, axis = 0) / m + (lambda_ / m) * w\n",
    "    dj_db = np.sum(errors) / m\n",
    "    \n",
    "    return dj_dw, dj_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e43da6c-57cd-4c6b-b7f4-3c2e1e7bf4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing compute_gradients function\n",
    "\n",
    "# Test 1: Empty Dataset\n",
    "X_test1 = np.array([])\n",
    "y_test1 = np.array([])\n",
    "w_test1 = np.array([])\n",
    "b_test1 = 0\n",
    "lambda_test1 = 1\n",
    "print(compute_gradients(X_test1.reshape(0, 0), y_test1, w_test1, b_test1, lambda_test1))  # Should return (0.0, 0.0)\n",
    "\n",
    "# Test 2: Single Training Example\n",
    "X_test2 = np.array([[1]])\n",
    "y_test2 = np.array([1])\n",
    "w_test2 = np.array([1])\n",
    "b_test2 = 1\n",
    "lambda_test2 = 1\n",
    "print(compute_gradients(X_test2, y_test2, w_test2, b_test2, lambda_test2))\n",
    "\n",
    "# Test 3: Single Feature\n",
    "X_test3 = np.array([[1], [2], [3]])\n",
    "y_test3 = np.array([1, 2, 3])\n",
    "w_test3 = np.array([1])\n",
    "b_test3 = 1\n",
    "lambda_test3 = 1\n",
    "print(compute_gradients(X_test3, y_test3, w_test3, b_test3, lambda_test3))\n",
    "\n",
    "# Test 4: Weights are Zero\n",
    "X_test4 = np.array([[1, 2], [3, 4]])\n",
    "y_test4 = np.array([1, 2])\n",
    "w_test4 = np.array([0, 0])\n",
    "b_test4 = 1\n",
    "lambda_test4 = 1\n",
    "print(compute_gradients(X_test4, y_test4, w_test4, b_test4, lambda_test4))\n",
    "\n",
    "# Test 5: High Regularization Factor\n",
    "X_test5 = np.array([[1, 2], [3, 4]])\n",
    "y_test5 = np.array([1, 2])\n",
    "w_test5 = np.array([1, 1])\n",
    "b_test5 = 1\n",
    "lambda_test5 = 100\n",
    "print(compute_gradients(X_test5, y_test5, w_test5, b_test5, lambda_test5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa9778f-ef4b-4605-b41d-c417d9323e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w_in, b_in, alpha, num_iters, lambda_):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    X : ndarray, shape [m, n] \n",
    "        Training Dataset of m examples and n features.\n",
    "    y: ndarray, shape [m,]    \n",
    "        Target values.\n",
    "    w_in: ndarray, shape [n,]    \n",
    "        Model Parameters (weights).\n",
    "    b_in: scalar  \n",
    "        Model Parameter (bias).\n",
    "    alpha: float\n",
    "        Learning rate.\n",
    "    num_iters: scalar\n",
    "        Number of iteration of gradient descent.\n",
    "\n",
    "    Returns:\n",
    "    w: ndarray, shape (n,)\n",
    "        Optimized model parameter (weights).\n",
    "    b: scalar\n",
    "        Optimized model parameter (bias).\n",
    "    J_history: ndarray\n",
    "        Cost value for different model parameters as gradient descent progresses.\n",
    "    \"\"\"\n",
    "    \n",
    "    w = copy.deepcopy(w_in)\n",
    "    b = b_in\n",
    "    J_history = []\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        dj_dw, dj_db = compute_gradients(X, y, w, b, lambda_)\n",
    "        b = b - alpha * dj_db\n",
    "        w = w- alpha * dj_dw\n",
    "\n",
    "        if i % (num_iters // 10) == 0:\n",
    "            J_history.append(compute_cost(X, y, w, b, lambda_))\n",
    "            print(f\"Iteration: {i}: Weights: {w}, Bias: {b}     Cost: {J_history[-1]}\")\n",
    "\n",
    "    return w, b, np.array(J_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8665cd0e-d8ee-47d5-835a-a40f409330ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the gradient_descent function\n",
    "\n",
    "# Test 1: Basic functionality test\n",
    "X = np.array([[1], [2], [3]])\n",
    "y = np.array([1, 2, 3])\n",
    "w = np.array([0])\n",
    "b = 0\n",
    "alpha = 0.01\n",
    "num_iters = 1000\n",
    "lambda_ = 0.1\n",
    "\n",
    "w_opt, b_opt, J_hist = gradient_descent(X, y, w, b, alpha, num_iters, lambda_)\n",
    "print(f\"Optimized Weights: {w_opt}, Optimized Bias: {b_opt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29e7610-853d-42d9-8793-0adbab413606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: (Edge Case Test) Empty Dataset\n",
    "X = np.array([]).reshape(0, 0)\n",
    "y = np.array([])\n",
    "w = np.array([])\n",
    "b = 0\n",
    "alpha = 0.01\n",
    "num_iters = 1000\n",
    "lambda_ = 0.1\n",
    "\n",
    "w_opt, b_opt, J_hist = gradient_descent(X, y, w, b, alpha, num_iters, lambda_)\n",
    "print(f\"Optimized Weights: {w_opt}, Optimized Bias: {b_opt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ee1cc4-ed55-4990-9a70-31801c430faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: (Edge Case Test) Single Data Point\n",
    "X = np.array([[2]])\n",
    "y = np.array([4])\n",
    "w = np.array([0])\n",
    "b = 0\n",
    "alpha = 0.01\n",
    "num_iters = 1000\n",
    "lambda_ = 0.1\n",
    "\n",
    "w_opt, b_opt, J_hist = gradient_descent(X, y, w, b, alpha, num_iters, lambda_)\n",
    "print(f\"Optimized Weights: {w_opt}, Optimized Bias: {b_opt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbc6bbc-c270-442c-959a-f0c8fa9f1e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 4: (Edge Case Test) High Regularization Factor\n",
    "X = np.array([[1, 2], [3, 4]])\n",
    "y = np.array([1, 2])\n",
    "w = np.array([0.5, 0.5])\n",
    "b = 1\n",
    "alpha = 0.01\n",
    "num_iters = 1000\n",
    "lambda_ = 100\n",
    "\n",
    "w_opt, b_opt, J_hist = gradient_descent(X, y, w, b, alpha, num_iters, lambda_)\n",
    "print(f\"Optimized Weights: {w_opt}, Optimized Bias: {b_opt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb54044-f3cf-4643-8324-b19d87eb66fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test 5: (Performance Test) Large Dataset\n",
    "X = np.random.rand(10000, 10)\n",
    "y = np.random.rand(10000)\n",
    "w = np.zeros(10)\n",
    "b = 0\n",
    "alpha = 0.01\n",
    "num_iters = 1000\n",
    "lambda_ = 0.1\n",
    "\n",
    "w_opt, b_opt, J_hist = gradient_descent(X, y, w, b, alpha, num_iters, lambda_)\n",
    "print(f\"Optimized Weights: {w_opt}, Optimized Bias: {b_opt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b08736b-e54b-43f3-b5b8-b049c0a68ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 6: Test with Different Learning Rates\n",
    "\n",
    "# Example with a low learning rate\n",
    "alpha_low = 0.001\n",
    "w_opt_low, b_opt_low, J_hist_low = gradient_descent(X, y, w, b, alpha_low, num_iters, lambda_)\n",
    "print(f\"Low Learning Rate - Optimized Weights: {w_opt_low}, Optimized Bias: {b_opt_low}\")\n",
    "\n",
    "# Example with a high learning rate\n",
    "alpha_high = 0.1\n",
    "w_opt_high, b_opt_high, J_hist_high = gradient_descent(X, y, w, b, alpha_high, num_iters, lambda_)\n",
    "print(f\"High Learning Rate - Optimized Weights: {w_opt_high}, Optimized Bias: {b_opt_high}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583002ef-9c5b-43c7-aeb0-d9b1cc574f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_score_normalize(X):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    X : ndarray, shape [m, n] \n",
    "        Training Dataset of m examples and n features.\n",
    "\n",
    "    Returns:\n",
    "    X_norm : ndarray, shape [m, n] \n",
    "        Normalized training Dataset of m examples and n features.\n",
    "    \"\"\"\n",
    "\n",
    "    mu = np.mean(X, axis = 0)\n",
    "    sigma = np.std(X, axis = 0)\n",
    "    \n",
    "    # Add a small epsilon to avoid division by zero\n",
    "    epsilon = 1e-8\n",
    "    X_norm = (X - mu) / (sigma + epsilon)\n",
    "    return X_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc85281-2091-4cd6-bfc8-92e20350845e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the data\n",
    "np.random.seed(42)\n",
    "X_train = np.arange(20).reshape([-1, 2])\n",
    "y_train = np.random.rand(10) + X_train[:, 0] + X_train[:, 1] - 3\n",
    "\n",
    "# Print the data for verification\n",
    "print(\"X_train:\\n\", X_train)\n",
    "print(\"y_train:\\n\", y_train)\n",
    "\n",
    "# Initialize parameters\n",
    "w_in = np.zeros(X_train.shape[1])\n",
    "b_in = 0.\n",
    "alpha = 0.00001\n",
    "num_iters = 100000\n",
    "lambda_ = 0.1\n",
    "\n",
    "# Perform gradient descent\n",
    "w, b, J_history = gradient_descent(X_train, y_train, w_in, b_in, alpha, num_iters, lambda_)\n",
    "\n",
    "# Print the optimized parameters\n",
    "print(f\"Optimized weights: {w}\")\n",
    "print(f\"Optimized bias: {b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa5fc0c-6453-4185-ad10-f48f0e1a6c5f",
   "metadata": {},
   "source": [
    "### Expected Outcome 201.42654620565548\n",
    "*Optimized weights*: [1.179191   0.68622532]\n",
    "\n",
    "*Optimized bias*: -0.49527794991438273\n",
    "\n",
    "*Last cost*: 0.48622795162209126"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
